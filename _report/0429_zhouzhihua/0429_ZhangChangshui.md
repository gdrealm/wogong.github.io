---
title: 杰出科学家周志华关于机器学习研究的讨论 
date: 2016-05-18
---

- 讲座时间：2016.4.29（周五）  8:30-9:10
- 讲座地点：五院1楼学术厅
- 授课人简介：周志华 zhouzh@nju.edu.cn

1973年11月生1996年6月南京大学计算机科学与技术系学士1998年6月南京大学计算机科学与技术系硕士2000年12月南京大学计算机科学与技术系博士2001年1月南京大学任教2002年破格晋升副教授2003年获国家杰出青年科学基金,任南京大学教授2004年获博士生导师资格2006年入选教育部长江学者特聘教授现任南京大学计算机科学与技术系副主任南京大学计算机软件新技术国家重点实验室常务副主任机器学习与数据挖掘研究所  (LAMDA)  所长南京大学、计算机系学术委员会委员国际计算机学会(ACM)  杰出科学家国际人工智能学会  (AAAI) 、国际电气电子工程师学会  (IEEE) 、国际模式识别学会  (IAPR)、国际工程技术学会  (IET/IEE)、中国计算机学会等学会的会士 (Fellow)。

主要从事人工智能、机器学习、数据挖掘、模式识别等领域的研究工作。主持多项科研课题，出版英文著作一部，主编文集多部，获发明专利十余项，在一流国际期刊和顶级国际会议发表论文百余篇，被引用万余次。

先后担任16种SCI(E)期刊编委，现任Frontiers  of Computer Science  执行主编，中国科学:  信息科学  等刊副主编，ACM  Transactions on Intelligent Systems and Technology、IEEE  Transactions on Neural Networks and Learning Systems等刊Associate  Editor，曾任科学通报副主编(2008-2014)，IEEE  Transactions on Knowledge and Data Engineering  (2008-2012) Associate Editor 等国际学术会议 ACML发起人及指导委员会主席，PAKDD、PRICAI指导委员会委员，IJCAI'15  顾问委员会委员暨机器学习主席，ADMA'12、PCM'13、PAKDD'14、ICDM'16  等的大会主席，PRICAI'08、SDM'13、ICDM'15  等的程序委员会主席，KDD'12、ICDM'14、KDD'16  研讨会主席，KDD'13、CIKM'14  讲座主席，MM'15 论坛主席，WCCI'16 专题主席，三十余次重要国际学术会议的领域主席，以及十余次国内学术会议主席；中国人工智能学会机器学习专业委员会主任(2006-2015)IEEE 计算机学会 Fellow 评选委员会委员现任中国计算机学会人工智能与模式识别专业委员会主任IEEE计算智能学会 数据挖掘与大数据分析技术委员会主席IEEE南京分部副主席IEEE计算机学会南京分会主席江苏省青联副主席南京大学首届青年学者联谊会会长 (2009-2014)。

## notes
### What is next?
深度学习
2006年，Hinton 深度学习 Nature
2012年，ImageNet大赛 CNN模型，10个百分点领先
2010年至今，6年热潮
最常用的深度学习模型：卷积神经网络 CNN

典型的深度学习模型：很深层的神经网络

提升模型复杂度 -\> 提升学习能力
- 增加隐层神经元数目、宽度，基函数个数
- 增加隐层数目，基函数迭代次数
增加过拟合风险（\<-使用大量数据），增加训练难度（\<-使用若干启发式诀窍）
误差梯度在多个隐层内传播

### 常用诀窍 tricks
1. 预训练+微调
   2. 预训练：监督逐层训练，每次训练一层隐节点
   3. 微调：预训练全部完成后，对全网络进行微调训练，通常使用BP算法
   4. R：可视为将大量参数分组，对每组先找到较好的局部配置，再全局寻优
2. 权共享 weight-sharing
   6. R：减少优化的参数
   7. 一组神经元使用相同的连接权值
3. Dropout
   9. R：可能降低Rademacher复杂度
   10. 在每轮训练时随机选择一些隐结点令其权值不被更新（下一轮可能被更新）
4. ReLU （Rectified Linear Units）
   12. R: 求导容易，可能缓解梯度消失现象
   13. 将 Sigmold 激活函数修改为修正线性函数：f(x)=max(0,x)

### 深度学习最重要的特质：表示学习、联合优化
传统做法：人工设计特征（Feature engineering）-\>学习分类
深度学习：学习特征（Representation learning）-\> 学习分类
（所谓 end-to-end learning 并非新东西）

NIPS 2015 的几个数字
深度学习仅占投稿数的9%，录用数的11%
然而，国内媒介上却只有深度学习的声音，什么地方除了问题？
至少，学界应该保持清醒

### 神经网络发展回顾
1940年代-萌芽期：M-P模型（1943），Hebb学习规则（1945）
1958左右-1969左右-繁荣期：感知机（1958），Adaline（1960）
1969年 Minsky & Papert "Perceptrons"
冰河期
1985左右-1995左右 -繁荣期：Hopfield（1983），BP（1986）
1995年左右 SVM及统计学习兴起
沉寂期
2010年左右-至今 繁荣期：深度学习

巧合？
1950年代中：现在电子计算机广泛应用
1980年代初：Intel x86 系列微处理器与内存条技术广泛应用
2000年代中：GPU、CPU集群广泛应用
R：神经网络是最易于利用新增计算能力的机器学习方法

- Hopfield(1983)
- BP走红（1986） P. Werbos 1974年完整提出（12年）
  8年
- SVM 1994年Machine learning发表，1998年文本分类重大进展

- Hinton 2006
- CNN 走红 2010   Y. LeCun 1998年完整提出 （12年）
  8年
  What？ （2018？2022？）
  追热门，赶潮流——永远落后
  现阶段的视野不应局限于“深度学习”

### Remark 1
What's next?
未必是深度学习，但应该是能有效利用GPU等计算设备的方法

AlphaGo
多种机器学习技术：深度学习，强化学习，半监督学习，蒙特卡罗树搜索

分布偏移，类别增长，属性退化，目标多样

国际上对AI发展的探讨
AAAI 主席报告（Presidential Address） 2016.02.14
Tom Dietterich
Steps Toward robust artificial intelligence
人工智能众多分支技术都取得了巨大发展
随着人工智能技术的发展，越来越多地面临“高风险应用”，因此必须有鲁棒的AI
AI系统必须面对事先完全不知道得世界
早期的世界：Known knowns 例如定理证明
然后 known unknowns 例如概率图模型
今后，unknown unknowns

### Remark 2 
What's next
从“封闭环境”走向“开放环境”是必然
开放环境下鲁棒机器学习技术是关键

谨慎参考，免受误导
谢谢